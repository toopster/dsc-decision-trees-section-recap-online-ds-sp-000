{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Recap\n",
    "\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "The key takeaways from this section include:\n",
    "\n",
    "* Decision trees can be used for both categorization and regression tasks\n",
    "* They are a powerful and interpretable technique for many machine learning problems (especially when combined with ensemble methods)\n",
    "* Decision trees are a form of Directed Acyclic Graphs (DAGs) - you traverse them in a specified direction, and there are no \"loops\" in the graphs to go backward\n",
    "* Algorithms for generating decision trees are designed to maximize the information gain from each split\n",
    "* A popular algorithm for generating decision trees is ID3 - the Iterative Dichotomiser 3 algorithm\n",
    "* There are several hyperparameters for decision trees to reduce overfitting - including maximum depth, minimum samples to split a node that is currently a leaf, minimum leaf sample size, maximum leaf nodes, and maximum features "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
